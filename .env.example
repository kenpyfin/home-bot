# MicroClaw configuration
# Copy to .env and fill in values: cp .env.example .env && microclaw setup

# Telegram (required unless using Discord only)
TELEGRAM_BOT_TOKEN=
BOT_USERNAME=

# LLM (anthropic, ollama, openai, openrouter, deepseek, google, etc.)
LLM_PROVIDER=anthropic
LLM_API_KEY=
LLM_MODEL=
# LLM_BASE_URL=

# Workspace
WORKSPACE_DIR=./workspace
TIMEZONE=UTC

# Orchestrator (plan-first architecture). Runs planning step before main agent loop.
# ORCHESTRATOR_ENABLED=true
# ORCHESTRATOR_MODEL=   # Optional: faster/cheaper model for planning; if empty, use main model

# Browser automation (optional). In Docker the image sets AGENT_BROWSER_PATH.
# AGENT_BROWSER_PATH=/usr/local/bin/agent-browser

# ORIGIN vault (optional). Paths relative to workspace_dir.
VAULT_ORIGIN_VAULT_PATH=shared/ORIGIN
VAULT_VECTOR_DB_PATH=shared/vault_db
# VAULT_ORIGIN_VAULT_REPO=https://github.com/you/ORIGIN.git

# Vault search: use EITHER command mode OR native mode (no Docker needed for command mode).
# Command mode uses built-in scripts at scripts/vault/ (or /app/scripts/vault/ in Docker).
# Docker (use venv with ChromaDB): VAULT_SEARCH_COMMAND='/app/workspace/shared/.venv-vault/bin/python /app/scripts/vault/query_vault.py "{query}"'
# Native (if chromadb in venv):    VAULT_SEARCH_COMMAND='workspace/shared/.venv-vault/bin/python scripts/vault/query_vault.py "{query}"'
# Or with system python + chromadb: VAULT_SEARCH_COMMAND='python3 scripts/vault/query_vault.py "{query}"'
# VAULT_INDEX_COMMAND: same pattern, e.g. /app/workspace/shared/.venv-vault/bin/python /app/scripts/vault/index_vault.py
# Native mode: embedding server + ChromaDB HTTP API (requires ChromaDB server running).
# VAULT_EMBEDDING_SERVER_URL=http://127.0.0.1:8080
# VAULT_VECTOR_DB_URL=http://localhost:8000
# VAULT_VECTOR_DB_COLLECTION=vault

# Git credentials for push inside container (optional). Enables git push from microclaw/sync.
# Use GitHub username and a Personal Access Token (PAT) for HTTPS repos.
# GIT_USERNAME=
# GIT_TOKEN=
